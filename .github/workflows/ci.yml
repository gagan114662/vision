name: CI Pipeline

on:
  workflow_dispatch:  # Allow manual triggering
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  schedule:
    - cron: '0 0 * * *'  # Daily run for regression tests

jobs:
  lint:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install ruff black isort mypy

      - name: Run linters
        run: |
          ruff check .
          black --check .
          isort --check-only .

  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.10', '3.11', '3.12']

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}

      - name: Cache dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov pytest-asyncio pytest-timeout

      - name: Run unit tests
        run: |
          pytest tests/ -v --cov=mcp --cov=agents --cov=portfolio --cov-report=xml

      - name: Upload coverage
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella

  registry-validation:
    runs-on: ubuntu-latest
    name: MCP Registry Validation
    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest

      - name: Run MCP Registry Validation with unittest
        run: |
          echo "Running MCP Registry validation with unittest..."
          python3 -m unittest tests.mcp.test_registry_validation
        env:
          PYTHONPATH: ${{ github.workspace }}

      - name: Run MCP Registry Validation with pytest
        run: |
          echo "Running MCP Registry validation with pytest..."
          python -m pytest tests/mcp/test_registry_validation.py -v
        env:
          PYTHONPATH: ${{ github.workspace }}

      - name: Check for registry drift
        run: |
          python -c "
          import sys
          sys.path.insert(0, '.')
          from tests.mcp.test_registry_validation import validate_registry_completeness

          result = validate_registry_completeness()
          if not result['all_valid']:
              print('Registry validation failed!')
              print(f\"Stub implementations: {result['stub_count']}\")
              print(f\"Missing schemas: {result['missing_schemas']}\")
              sys.exit(1)
          else:
              print('Registry validation passed!')
          "

  integration-tests:
    runs-on: ubuntu-latest
    name: Integration Tests
    services:
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-asyncio

      - name: Run integration tests
        env:
          REDIS_URL: redis://localhost:6379
          PYTHONPATH: ${{ github.workspace }}
        run: |
          # Run circuit breaker integration tests
          python -m pytest test_circuit_breaker_integration.py -v

          # Run QuantConnect integration tests
          python -m pytest test_quantconnect_live_validation.py -v --timeout=120

          # Run core MCP servers validation
          python -m pytest test_core_mcp_servers_validation.py -v

          # Run streaming pipeline tests
          python -m pytest test_streaming_pipeline.py -v

      - name: Validate circuit breakers (non-simulated)
        run: |
          python validate_circuit_breaker_production.py
        continue-on-error: true  # Don't fail CI if production validation fails

  backtest-validation:
    runs-on: ubuntu-latest
    name: Backtest Validation
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install lean docker

      - name: Setup Docker
        uses: docker/setup-buildx-action@v2

      - name: Run Real QuantConnect Backtest Validation
        env:
          QC_USER_ID: ${{ secrets.QC_USER_ID }}
          QC_API_TOKEN: ${{ secrets.QC_API_TOKEN }}
          QUANTCONNECT_USER_ID: ${{ secrets.QC_USER_ID }}
          QUANTCONNECT_API_TOKEN: ${{ secrets.QC_API_TOKEN }}
        run: |
          echo "Running real QuantConnect backtest validation..."
          python test_quantconnect_real_backtest.py

      - name: Verify Non-Synthetic Results
        run: |
          python -c "
          import json
          import sys
          from pathlib import Path

          results_dir = Path('./lean/results')
          if not results_dir.exists():
              print('ERROR: No backtest results found')
              sys.exit(1)

          # Find latest result
          result_dirs = sorted([d for d in results_dir.iterdir() if d.is_dir()])
          if not result_dirs:
              print('ERROR: No result directories found')
              sys.exit(1)

          latest_dir = result_dirs[-1]
          print(f'Checking results in: {latest_dir}')

          # Check for real statistics (not synthetic)
          stats_files = list(latest_dir.glob('*-statistics.json'))
          if not stats_files:
              print('ERROR: No statistics file found')
              sys.exit(1)

          with open(stats_files[0]) as f:
              stats = json.load(f)

          # Validate this is real data
          if 'IsSimulated' in stats and stats['IsSimulated']:
              print('ERROR: Results are synthetic!')
              sys.exit(1)

          # Check for actual trades
          if 'TotalPerformance' in stats:
              total_trades = stats['TotalPerformance'].get('TotalTrades', 0)
              if total_trades == 0:
                  print('WARNING: No trades executed in backtest')
              else:
                  print(f'✓ Real backtest executed with {total_trades} trades')
                  print(f\"  Sharpe Ratio: {stats['TotalPerformance'].get('SharpeRatio', 'N/A')}\")
          else:
              print('ERROR: Invalid statistics format')
              sys.exit(1)

          print('✓ QuantConnect backtest validation PASSED')
          "

  performance-benchmarks:
    runs-on: ubuntu-latest
    name: Performance Benchmarks
    if: github.event_name == 'push'

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest-benchmark

      - name: Run performance benchmarks
        run: |
          python -c "
          import time
          import sys
          sys.path.insert(0, '.')

          # Test continuous batching performance
          from performance.continuous_batch_processor import ContinuousBatchProcessor, BatchConfig

          def test_batch(items):
              return [item * 2 for item in items]

          processor = ContinuousBatchProcessor(test_batch, BatchConfig())

          start = time.time()
          for i in range(1000):
              processor.submit(i)
          elapsed = time.time() - start

          print(f'Batch processing: 1000 items in {elapsed:.2f}s')
          print(f'Throughput: {1000/elapsed:.1f} items/s')

          # Test cache performance
          from performance.multi_tier_cache import MultiTierCache, CacheConfig

          cache = MultiTierCache(CacheConfig())

          start = time.time()
          for i in range(1000):
              cache.get(f'key_{i}')
              cache.put(f'key_{i}', f'value_{i}')
          elapsed = time.time() - start

          print(f'Cache operations: 2000 ops in {elapsed:.2f}s')
          print(f'Throughput: {2000/elapsed:.1f} ops/s')
          "

      - name: Store benchmark results
        uses: actions/upload-artifact@v3
        with:
          name: performance-benchmarks
          path: |
            circuit_breaker_validation.json
            performance_*.json

  security-scan:
    runs-on: ubuntu-latest
    name: Security Scanning

    steps:
      - uses: actions/checkout@v3

      - name: Run Trivy security scan
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-results.sarif'

      - name: Upload Trivy results to GitHub Security tab
        uses: github/codeql-action/upload-sarif@v2
        with:
          sarif_file: 'trivy-results.sarif'

      - name: Check for secrets
        run: |
          pip install detect-secrets
          detect-secrets scan --baseline .secrets.baseline

  notify-results:
    runs-on: ubuntu-latest
    needs: [lint, test, registry-validation, integration-tests]
    if: always()

    steps:
      - name: Check job results
        run: |
          echo "Lint: ${{ needs.lint.result }}"
          echo "Test: ${{ needs.test.result }}"
          echo "Registry Validation: ${{ needs.registry-validation.result }}"
          echo "Integration Tests: ${{ needs.integration-tests.result }}"

          if [[ "${{ needs.lint.result }}" == "failure" || \
                "${{ needs.test.result }}" == "failure" || \
                "${{ needs.registry-validation.result }}" == "failure" || \
                "${{ needs.integration-tests.result }}" == "failure" ]]; then
            echo "One or more critical jobs failed!"
            exit 1
          fi

          echo "All critical jobs passed!"